{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "A WebsiteData object to make code generaizable and easier to use with just URL's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing Jupyter notebook from getTextAndFiles.ipynb\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import re\n",
    "import nbimporter\n",
    "import getTextAndFiles as helper\n",
    "\n",
    "class WebsiteData:\n",
    "    \n",
    "    def __init__(self, url, nodeParams, keyValues):\n",
    "        html_page = urllib.request.urlopen(url)\n",
    "        self.soup = BeautifulSoup(html_page, \"lxml\")\n",
    "        #self.URL =  self.getUsefulParentURL(url)\n",
    "        self.URL = url\n",
    "        self.links = self.getUsefulLinks()\n",
    "        self.nodeParameters = nodeParams\n",
    "        self.keyValueLists = keyValues\n",
    "        self.JSONNode = {}\n",
    "            \n",
    "    #Need to adjust parent URL to ensure that it can be used to filter the links in the page\n",
    "    def getUsefulParentURL(self, parentURL):\n",
    "        #list of possible correct endings for the parentURL\n",
    "        possibleParentEnds = ['.org', '.edu', '.com']\n",
    "    \n",
    "        for end in possibleParentEnds:\n",
    "            #attempts to split parentURL by one of the endings\n",
    "            parentURLSplit = parentURL.split(end)\n",
    "        \n",
    "            #if the split is successful then the parentURL is reassigned to the string before the split + the ending\n",
    "            if len(parentURLSplit) > 1:\n",
    "                parentURL = parentURLSplit[0] + end\n",
    "                #print('accessed')\n",
    "                return parentURL\n",
    "            else:\n",
    "                return 'invalid string'\n",
    "            \n",
    "    def urlJoin(self, parentURL, end):\n",
    "        if end[0] == '/':\n",
    "            return parentURL + end\n",
    "        else:\n",
    "            return parentURL + '/' + end\n",
    "            \n",
    "    #Finds all links in the page that are within the website and not malformed by ensuring the parentURL is in the URL.\n",
    "    def getUsefulLinks(self):\n",
    "        links = helper.getLinks(self.URL, 5)\n",
    "    \n",
    "        #if self.soup.find_all('a') != None:\n",
    "         #   for link in self.soup.find_all('a'):\n",
    "          #      if link != None and link.get('href') != None and self.URL in link.get('href'):\n",
    "                    #print(link.get('href'))\n",
    "           #         links.append(link.get('href'))\n",
    "        \n",
    "        #links.append(self.URL)\n",
    "        #return links\n",
    "        #pattern = re.compile(\"((http|ftp)s?://.*?)\")\n",
    "    \n",
    "        #for child in self.soup.recursiveChildGenerator():\n",
    "        #    name = getattr(child, \"name\", None)\n",
    "        #    if name is not None:\n",
    "        #        if name == 'a':\n",
    "        #            if child.get('href') != None:\n",
    "        #                current_link = child.get('href')\n",
    "        #                if not pattern.match(current_link):\n",
    "        #                    current_link = self.urlJoin(self.URL, current_link)\n",
    "        #                if self.URL in current_link:\n",
    "        #                    links.append(current_link)\n",
    "        #links.append(self.URL)\n",
    "        return links\n",
    "    \n",
    "    #Finds all tags within a BeautifulSoup object that's text contains a certain keyword and returns a list of these tags.\n",
    "    def findTags(self, soup, keyWord):\n",
    "        tagList = []\n",
    "        \n",
    "        for elem in soup(text=re.compile(keyWord)):\n",
    "            tagList.append(elem.parent)\n",
    "            \n",
    "        return tagList\n",
    "\n",
    "    #Utilizes a BeautifulSoup object (soup) to scrape all text that contains the given keyword. Filters for certain tags and does\n",
    "    #not take text from solely header (h) tags, instead takes text from next filtered tag after the header. Returns \n",
    "    def keyWordScraper(self, soup, keyWord):\n",
    "        searchTxt = ''\n",
    "        correctName = ['p', 'li', 'table', 'ul']\n",
    "        tagList = self.findTags(soup, keyWord)\n",
    "    \n",
    "        #This loop goes through each tag and rips the text from that tag. If that tag is a header, then the text from the next \n",
    "        #useful tag is ripped instead. All text is saved in searchTxt\n",
    "        for tag in tagList:\n",
    "            if 'h' in tag.name:\n",
    "                typeTag = type(soup.find('li'))\n",
    "                current = tag.next_sibling\n",
    "                while current != None and  not isinstance(current, typeTag) and current.name not in correctName:\n",
    "                    current = current.next_sibling\n",
    "                if current != None:\n",
    "                    if 'ul' in current.name:\n",
    "                        for li in current.findAll('li'):\n",
    "                            searchTxt = searchTxt + li.text\n",
    "                    else:\n",
    "                        searchTxt = searchTxt + current.text\n",
    "            else:\n",
    "                if(tag.name in correctName):\n",
    "                    searchTxt = searchTxt + tag.text\n",
    "                \n",
    "        return searchTxt\n",
    "    \n",
    "    #Applies the keyWordScraper function to a BeautifulSoup object(soup) for a list of keywords. Returns the text found.\n",
    "    def mapKeywordScraper(self, soup, keyWords):\n",
    "        foundTxt = ''\n",
    "    \n",
    "        for keyWord in keyWords:\n",
    "            foundTxt = foundTxt + self.keyWordScraper(soup, keyWord)\n",
    "        #print(foundTxt)\n",
    "        return foundTxt\n",
    "    \n",
    "    #this method takes a list of links and applies the scraping method to that list. \n",
    "    #WARNING: this method may take a while since it goes through every link, give it time to run before terminating it.\n",
    "    def mapURLs(self, keywords):\n",
    "        foundTxt = ''\n",
    "        for link in self.links:\n",
    "#            try:\n",
    "#                new_page = urllib.request.urlopen(link)\n",
    "#                new_soup = BeautifulSoup(new_page, \"lxml\")\n",
    "#                foundTxt = foundTxt + self.mapKeywordScraper(new_soup, keywords)\n",
    "#                print(foundTxt)\n",
    "#            except urllib.error.HTTPError as err:\n",
    "#                if err.code == 404:\n",
    "#                    testedLinks[links[i]] = 'Page not found! (404 Error)' \n",
    "#                elif err.code == 403:\n",
    "#                    testedLinks[links[i]] = 'Access denied! (403 Error)'\n",
    "#                else:\n",
    "#                    testedLinks[links[i]] = 'Something happened! Error code: ' + err.code\n",
    "            new_page = urllib.request.urlopen(link)\n",
    "            new_soup = BeautifulSoup(new_page, \"lxml\")\n",
    "            foundTxt = foundTxt + self.mapKeywordScraper(new_soup, keywords)\n",
    "        return foundTxt\n",
    "    \n",
    "    #creates JSON object\n",
    "    def createJsonNode(self):\n",
    "        for x in range(0, len(self.keyValueLists)):\n",
    "            self.JSONNode[self.nodeParameters[x]] = [self.mapURLs(self.keyValueLists[x])]\n",
    "            print(self.JSONNode)\n",
    "        return self.JSONNode\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
