{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Trying to Condense Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This code opens a webpage and attempts to get every link from within it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Problems with the below code: HTTPError 403, useful information may be found on the pages of links (this can be implemented fairly easily but could increase run time dramatically), still need to handle pdf's \n",
    "\n",
    "Thao said that there was a work around for 403's??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import re\n",
    "import lxml.html\n",
    " \n",
    "originalURL = 'https://www.richland2.org/charterhigh/'\n",
    "html_page = urllib.request.urlopen(originalURL)\n",
    "\n",
    "soup = BeautifulSoup(html_page, \"lxml\")\n",
    "\n",
    "#Need to adjust parent URL to ensure that it can be used to filter the links in the page\n",
    "def getUsefulParentURL(parentURL):\n",
    "    #list of possible correct endings for the parentURL\n",
    "    possibleParentEnds = ['.org', '.edu', '.com']\n",
    "    \n",
    "    for end in possibleParentEnds:\n",
    "        #attempts to split parentURL by one of the endings\n",
    "        parentURLSplit = parentURL.split(end)\n",
    "        \n",
    "        #if the split is successful then the parentURL is reassigned to the string before the split + the ending\n",
    "        if len(parentURLSplit) > 1:\n",
    "            parentURL = parentURLSplit[0] + end\n",
    "            return parentURL\n",
    "        \n",
    "parentURL = getUsefulParentURL(originalURL)\n",
    "\n",
    "#Finds all links in the page that are within the website and not malformed by ensuring the parentURL is in the URL.\n",
    "def getUsefulLinks():\n",
    "    links = []\n",
    "    \n",
    "    #if soup.find_all('a') != None:\n",
    "     #   for link in soup.find_all('a'):\n",
    "      #      if link != None and link.get('href') != None and parentURL in link.get('href'):\n",
    "       #         #print(link.get('href'))\n",
    "        #        links.append(link.get('href'))\n",
    "                \n",
    "    \n",
    "    #links.append(originalURL)\n",
    "    #return links\n",
    "    #connection = urllib.urlopen(originalURL)\n",
    "    \n",
    "    #dom =  lxml.html.fromstring(connection.read())\n",
    "\n",
    "    #for link in dom.xpath('//a/@href'): # select the url in href for all a tags(links)\n",
    "     #   print link\n",
    "    #return links\n",
    "    pattern = re.compile(\"((http|ftp)s?://.*?)\")\n",
    "    for child in soup.recursiveChildGenerator():\n",
    "        name = getattr(child, \"name\", None)\n",
    "        if name is not None:\n",
    "            if name == 'a':\n",
    "                if child.get('href') != None:\n",
    "                    current_link = child.get('href')\n",
    "                    if not pattern.match(current_link):\n",
    "                        current_link = urlJoin(parentURL, current_link)\n",
    "                    if parentURL in current_link:\n",
    "                        links.append(current_link)\n",
    "    links.append(parentURL)\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def urlJoin(parentURL, end):\n",
    "    if end[0] == '/':\n",
    "        return parentURL + end\n",
    "    else:\n",
    "        return parentURL + '/' + end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "https://www.richland2.org/CharterHigh/\n",
      "\n",
      "\n",
      "https://www.richland2.org/CharterHigh/\n",
      "\n",
      "https://www.richland2.org/CharterHigh/About-Us\n",
      "\n",
      "https://www.richland2.org/CharterHigh/School-Information\n",
      "\n",
      "https://www.richland2.org/CharterHigh/School-Information/Graduation-2017\n",
      "\n",
      "https://www.richland2.org/CharterHigh/School-Information/Lab-Hours\n",
      "\n",
      "https://www.richland2.org/CharterHigh/School-Information/Calendar\n",
      "\n",
      "https://www.richland2.org/CharterHigh/School-Information/Frequently-Asked-Questions\n",
      "\n",
      "https://www.richland2.org/CharterHigh/School-Information/Directions\n",
      "\n",
      "https://www.richland2.org/CharterHigh/School-Information/School-Board\n",
      "\n",
      "https://www.richland2.org/CharterHigh/School-Information/School-Improvement-Council\n",
      "\n",
      "https://www.richland2.org/CharterHigh/Contact-Us\n",
      "\n",
      "\n",
      "https://www.richland2.org/CharterHigh/News\n",
      "\n",
      "https://www.richland2.org/CharterHigh/Careers\n",
      "\n",
      "https://www.richland2.org/CharterHigh/Student-Registration\n",
      "\n",
      "\n",
      "https://www.richland2.org/CharterHigh/Special-Pages/Site-Translations\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "https://www.richland2.org/news\n",
      "\n",
      "https://www.richland2.org/CharterHigh/News/We-re-Hiring-a-Principal\n",
      "\n",
      "\n",
      "https://www.richland2.org/CharterHigh/News/Welcome-to-Our-New-Website!\n",
      "\n",
      "\n",
      "\n",
      "https://www.richland2.org/About-Our-School/Calendar\n",
      "\n",
      "https://www.richland2.org/CharterHigh/About-Our-School/Calendar\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "https://www.richland2.org/CharterHigh/About-Our-School/Calendar\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "https://www.richland2.org/CharterHigh/About-Our-School/Calendar\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "https://www.richland2.org/CharterHigh/About-Our-School/Calendar\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "https://www.richland2.org/CharterHigh/About-Our-School/Calendar\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "https://www.richland2.org/CharterHigh/About-Our-School/Calendar\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "https://www.richland2.org/~\n",
      "\n",
      "\n",
      "https://www.richland2.org/accessibility.aspx\n",
      "\n",
      "\n",
      "\n",
      "https://www.richland2.org\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def allChildrenNames():\n",
    "    pattern = re.compile(\"((http|ftp)s?://.*?)\")\n",
    "    \n",
    "    for child in soup.recursiveChildGenerator():\n",
    "        name = getattr(child, \"name\", None)\n",
    "        if name is not None:\n",
    "            if name == 'a':\n",
    "                if child.get('href') != None:\n",
    "                    current_link = child.get('href')\n",
    "                    if not pattern.match(current_link):\n",
    "                        current_link = urlJoin(parentURL, current_link)\n",
    "                    if parentURL in current_link:\n",
    "                        print(current_link)\n",
    "        elif not child.isspace(): # leaf node, don't print spaces\n",
    "            print('')\n",
    "\n",
    "allChildrenNames()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def findKeyWordTextNeighborTesterWithNameCheck(soup, keyWord):\n",
    "    searchTxt = ''\n",
    "    tagLst = []\n",
    "    correctName = ['p', 'li', 'table']\n",
    "    \n",
    "    #This loop goes through all the text in every tag. If the text contains a keyword, then it puts that text's tag in the\n",
    "    #tagLst.\n",
    "    for elem in soup(text=re.compile(keyWord)):\n",
    "        tagLst.append(elem.parent)\n",
    "    \n",
    "    #This loop goes through each tag and rips the text from that tag. If that tag is a header, then the text from the next \n",
    "    #useful tag is ripped instead. All text is saved in searchTxt\n",
    "    for tag in tagLst:\n",
    "        if 'h' in tag.name:\n",
    "            typeTag = type(soup.find('li'))\n",
    "            current = tag.next_sibling\n",
    "            while current != None and  not isinstance(current, typeTag) and current.name not in correctName:\n",
    "                current = current.next_sibling\n",
    "            if current != None:\n",
    "                searchTxt = searchTxt + current.text\n",
    "        else:\n",
    "            if(tag.name in correctName):\n",
    "                searchTxt = searchTxt + tag.text\n",
    "            \n",
    "    return searchTxt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This method applies the scraping method for each keyword in a list of keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def keyWordTextTester(keyWords, soup):\n",
    "    foundTxt = ''\n",
    "    \n",
    "    for keyWord in keyWords:\n",
    "        foundTxt = foundTxt + findKeyWordTextNeighborTesterWithNameCheck(soup, keyWord)\n",
    "    \n",
    "    return foundTxt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Lists for keywords for each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "mission = [' vision ', 'Vision ', 'our purpose', ' cause ', ' objectives ', ' mission', ' vision: ', ' mission: ', 'Mission']\n",
    "\n",
    "curriculum = [ 'program', 'methods', 'pedagogy', 'approach', 'model', 'curriculum', 'academics', 'degree']\n",
    "\n",
    "philosophy = ['beliefs', 'principles', 'creed', 'Values', 'philosophy', 'moral']\n",
    "\n",
    "history = ['story', 'background', 'founded', 'established']\n",
    "\n",
    "target = ['gifted', 'at-risk', 'ethnic background', 'target population', 'target audience']\n",
    "\n",
    "resources = ['ESL', 'tutoring', 'after-school programs', 'available resources', 'services', 'opportunities', 'opportunity']\n",
    "\n",
    "orgfactor = ['statistics', 'API', 'environment', 'ratio', 'average', 'female', 'fund', 'community']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Once we have the useful links we need to go loop through each one and parse that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#this method takes a list of links and applies the scraping method to that list. \n",
    "#WARNING: this method may take a while since it goes through every link, give it time to run before terminating it.\n",
    "def mapURLs(links, keywords):\n",
    "    foundTxt = ''\n",
    "    for link in links:\n",
    "        try:\n",
    "            new_page = urllib.request.urlopen(link)\n",
    "            new_soup = BeautifulSoup(new_page, \"lxml\")\n",
    "            foundTxt = foundTxt + keyWordTextTester(keywords, new_soup)\n",
    "        except urllib.error.HTTPError as err:\n",
    "            if err.code == 404:\n",
    "                foundTxt = 'Page not found! (404 Error)' \n",
    "            elif err.code == 403:\n",
    "                foundTxt = 'Access denied! (403 Error)'\n",
    "            else:\n",
    "                foundTxt = 'Something happened! Error code: ' + err.code\n",
    "        \n",
    "    return foundTxt\n",
    "\n",
    "#print(mapURLs(links))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Creating a School Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "school = {'mission' : [], 'curriculum': [], 'philosophy' : [], 'history' : [], 'target' : [], 'resources' : [],\n",
    "          'organizational_factors' : []}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Next method is purely for timing purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def wrapper(func, *args):\n",
    "    def wrapped():\n",
    "        return func(*args)\n",
    "    return wrapped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Code to run the parser for every keyword list and then adding the scraped text to the corresponding array in the school object.\n",
    "# Warning this code takes a LONG time, don't give up on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "links = getUsefulLinks()\n",
    "\n",
    "def addTextToSchoolObject(school):\n",
    "    keywordLists = [mission, curriculum, philosophy, history, target, resources, orgfactor]\n",
    "    schoolArrayNames = ['mission', 'curriculum', 'philosophy', 'history', 'target', 'resources' , 'organizational_factors']\n",
    "\n",
    "    for x in range(0, len(keywordLists)):\n",
    "        school[schoolArrayNames[x]] += [mapURLs(links, keywordLists[x])]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112.27559246280498"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import timeit\n",
    "\n",
    "wrapped = wrapper(addTextToSchoolObject, school)\n",
    "timeit.timeit(wrapped, number=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "yikes that's a long time..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mission': ['Page not found! (404 Error)'], 'curriculum': ['Page not found! (404 Error)'], 'philosophy': ['Page not found! (404 Error)'], 'history': ['Page not found! (404 Error)'], 'target': ['Page not found! (404 Error)'], 'resources': ['Page not found! (404 Error)Richland School District Two is fully committed to providing information and services to the community that meet a variety of communication needs. \\xa0The district is continuously engaged in making improvements to this site to ensure everyone in our community has equal access to the tools and information provided on this site.'], 'organizational_factors': ['Page not found! (404 Error)Richland School District Two is fully committed to providing information and services to the community that meet a variety of communication needs. \\xa0The district is continuously engaged in making improvements to this site to ensure everyone in our community has equal access to the tools and information provided on this site.In partnership with our community, Richland School District Two prepares all students for success by providing meaningful, challenging and engaging learning experiences.In partnership with our community, Richland School District Two prepares all students for success by providing meaningful, challenging and engaging learning experiences.']}\n"
     ]
    }
   ],
   "source": [
    "print(school)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
