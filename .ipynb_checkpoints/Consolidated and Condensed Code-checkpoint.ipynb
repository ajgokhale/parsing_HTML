{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Trying to Condense Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This code opens a webpage and attempts to get every link from within it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Problems with the below code: HTTPError 403\n",
    "\n",
    "Thao said that there was a work around for 403's??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import re\n",
    " \n",
    "originalURL = 'https://www.polk.edu/lakeland-gateway-to-college-high-school/'\n",
    "html_page = urllib.request.urlopen('https://www.polk.edu/lakeland-gateway-to-college-high-school/about')\n",
    "\n",
    "soup = BeautifulSoup(html_page, \"lxml\")\n",
    "\n",
    "#Need to adjust parent URL to ensure that it can be used to filter the links in the page\n",
    "def getUsefulParentURL(parentURL):\n",
    "    #list of possible correct endings for the parentURL\n",
    "    possibleParentEnds = ['.org', '.edu', '.com']\n",
    "    \n",
    "    for end in possibleParentEnds:\n",
    "        #attempts to split parentURL by one of the endings\n",
    "        parentURLSplit = parentURL.split(end)\n",
    "        \n",
    "        #if the split is successful then the parentURL is reassigned to the string before the split + the ending\n",
    "        if len(parentURLSplit) > 1:\n",
    "            parentURL = parentURLSplit[0] + end\n",
    "            return parentURL\n",
    "        \n",
    "parentURL = getUsefulParentURL(originalURL)\n",
    "\n",
    "#Finds all links in the page that are within the website and not malformed by ensuring the parentURL is in the URL.\n",
    "def getUsefulLinks():\n",
    "    links = []\n",
    "    \n",
    "    if soup.find_all('a') != None:\n",
    "        for link in soup.find_all('a'):\n",
    "            if link != None and link.get('href') != None and parentURL in link.get('href'):\n",
    "                #print(link.get('href'))\n",
    "                links.append(link.get('href'))\n",
    "    links.append(originalURL)\n",
    "    return links\n",
    "\n",
    "#getUsefulLinks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This method goes through tags and extracts the text from useful tags based on keywords. Filters tags based on the tags in the correct tag list to leave out non-text elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def findKeyWordTextNeighborTesterWithNameCheck(soup, keyWord):\n",
    "    searchTxt = ''\n",
    "    tagLst = []\n",
    "    correctName = ['p', 'li', 'table']\n",
    "    \n",
    "    #This loop goes through all the text in every tag. If the text contains a keyword, then it puts that text's tag in the\n",
    "    #tagLst.\n",
    "    for elem in soup(text=re.compile(keyWord)):\n",
    "        tagLst.append(elem.parent)\n",
    "    \n",
    "    #This loop goes through each tag and rips the text from that tag. If that tag is a header, then the text from the next \n",
    "    #useful tag is ripped instead. All text is saved in searchTxt\n",
    "    for tag in tagLst:\n",
    "        if 'h' in tag.name:\n",
    "            typeTag = type(soup.find('li'))\n",
    "            current = tag.next_sibling\n",
    "            while not isinstance(current, typeTag) and current.name not in correctName:\n",
    "                current = current.next_sibling\n",
    "            searchTxt = searchTxt + current.text\n",
    "        else:\n",
    "            if(tag.name in correctName):\n",
    "                searchTxt = searchTxt + tag.text\n",
    "            \n",
    "    return searchTxt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This method applies the scraping method for each keyword in a list of keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def keyWordTextTester(keyWords, soup):\n",
    "    foundTxt = ''\n",
    "    \n",
    "    for keyWord in keyWords:\n",
    "        foundTxt = foundTxt + findKeyWordTextNeighborTesterWithNameCheck(soup, keyWord)\n",
    "    \n",
    "    return foundTxt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Lists for keywords for each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "mission = [' vision ', 'Vision ', 'our purpose', ' cause ', ' objectives ', ' mission', ' vision: ', ' mission: ', 'Mission']\n",
    "\n",
    "curriculum = [ 'program', 'methods', 'pedagogy', 'approach', 'model', 'curriculum', 'academics', 'degree']\n",
    "\n",
    "philosophy = ['beliefs', 'principles', 'creed', 'Values', 'philosophy', 'moral']\n",
    "\n",
    "history = ['story', 'background', 'founded', 'established']\n",
    "\n",
    "target = ['gifted', 'at-risk', 'ethnic background', 'target population', 'target audience']\n",
    "\n",
    "resources = ['ESL', 'tutoring', 'after-school programs', 'available resources', 'services', 'opportunities', 'opportunity']\n",
    "\n",
    "orgfactor = ['statistics', 'API', 'environment', 'ratio', 'average', 'female', 'fund', 'community']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Once we have the useful links we need to go loop through each one and parse that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#this method takes a list of links and applies the scraping method to that list. \n",
    "#WARNING: this method may take a while since it goes through every link, give it time to run before terminating it.\n",
    "def mapURLs(links, keywords):\n",
    "    foundTxt = ''\n",
    "    for link in links:\n",
    "        new_page = urllib.request.urlopen(link)\n",
    "        new_soup = BeautifulSoup(new_page, \"lxml\")\n",
    "        foundTxt = foundTxt + keyWordTextTester(keywords, new_soup)\n",
    "    return foundTxt\n",
    "\n",
    "#print(mapURLs(links))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Creating a School Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "school = {'mission' : [], 'curriculum': [], 'philosophy' : [], 'history' : [], 'target' : [], 'resources' : [],\n",
    "          'organizational_factors' : []}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Next method is purely for timing purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def wrapper(func, *args):\n",
    "    def wrapped():\n",
    "        return func(*args)\n",
    "    return wrapped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Code to run the parser for every keyword list and then adding the scraped text to the corresponding array in the school object.\n",
    "# Warning this code takes a LONG time, don't give up on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "links = getUsefulLinks()\n",
    "\n",
    "def addTextToSchoolObject(school):\n",
    "    keywordLists = [mission, curriculum, philosophy, history, target, resources, orgfactor]\n",
    "    schoolArrayNames = ['mission', 'curriculum', 'philosophy', 'history', 'target', 'resources' , 'organizational_factors']\n",
    "\n",
    "    for x in range(0, len(keywordLists)):\n",
    "        school[schoolArrayNames[x]] += [mapURLs(links, keywordLists[x])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63.895838818821574"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped = wrapper(addTextToSchoolObject, school)\n",
    "timeit.timeit(wrapped, number=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "yikes that's a long time..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
